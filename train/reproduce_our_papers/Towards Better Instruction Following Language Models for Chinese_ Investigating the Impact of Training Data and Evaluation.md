### [Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation](https://github.com/LianjiaTech/BELLE/blob/main/docs/Towards%20Better%20Instruction%20Following%20Language%20Models%20for%20Chinese.pdf)

# Paper Introduction

This paper investigates the performance of models trained on currently available public instruction data. We conducted quantitative evaluations on our own 1,000-sample test set. Additionally, to improve model performance and training/inference efficiency in the Chinese domain, we expanded the LLaMA vocabulary and performed secondary pretraining on 3.4 billion Chinese tokens.

The public instruction datasets we used include:

1. Stanford Alpaca Chinese and English data generated by GPT-3.5  
2. Alpaca Chinese and English data generated by GPT-4  
3. Real user-ChatGPT conversation data from sharegpt  

We focus on exploring how types of training data influence model performance. Specifically, we examine factors such as the quantity, quality, and language distribution of the training data.

Experimental results are as follows:

<table>
  <tr>
    <td> Factor </td>
    <td> Base model </td>
    <td> Training data </td>
    <td> Score_w/o_others </td>
  <tr>
    <td rowspan="2">Vocabulary expansion</td>
    <td> LLaMA-7B-EXT </td>
    <td> zh(alpaca-3.5&4) + sharegpt </td>
    <td> 0.670 </td>
  </tr>
  <tr>
    <td> LLaMA-7B </td>
    <td> zh(alpaca-3.5&4) + sharegpt </td>
    <td> 0.652</td>
  </tr>
  <tr>
    <td rowspan="2">Data quality</td>
    <td> LLaMA-7B-EXT </td>
    <td> zh(alpaca-3.5) </td>
    <td> 0.642 </td>
  </tr>
  <tr>
    <td> LLaMA-7B-EXT </td>
    <td> zh(alpaca-4) </td>
    <td> 0.693 </td>
  </tr>
  <tr>
    <td rowspan="4">Language distribution</td>
    <td> LLaMA-7B-EXT </td>
    <td> cn(alpaca-3.5&4) </td>
    <td> 0.679 </td>
  </tr>
  <tr>
    <td> LLaMA-7B-EXT </td>
    <td> en(alpaca-3.5&4) </td>
    <td> 0.659 </td>
  </tr>
  <tr>
    <td> LLaMA-7B-EXT </td>
    <td> zh(alpaca-3.5&4) + sharegpt </td>
    <td> 0.670 </td>
  </tr>
  <tr>
    <td> LLaMA-7B-EXT </td>
    <td> en(alpaca-3.5&4) + sharegpt </td>
    <td> 0.668 </td>
  </tr>
  <tr>
    <td rowspan="2">Data scale</td>
    <td> LLaMA-7B-EXT </td>
    <td> zh(alpaca-3.5&4) + sharegpt </td>
    <td> 0.670 </td>
  </tr>
  <tr>
    <td> LLaMA-7B-EXT </td>
    <td> zh(alpaca-3.5&4) + sharegpt <br>+ BELLE-0.5M-CLEAN</td>
    <td> 0.762</td>
  </tr>
  <tr>
    <td>-</td>
    <td>ChatGPT</td>
    <td>-</td>
    <td>0.824</td>
</table>

**BELLE-0.5M-CLEAN** is a 0.5M-sample dataset cleaned from our internal 2.3M instruction samples. It includes both single-turn and multi-turn dialogues and is not the same as the previously released 0.5M dataset. This dataset has not yet been open-sourced, but we have released the best-performing model from the experiments (score 0.762) on [Hugging Face](https://huggingface.co/BelleGroup/BELLE-on-Open-Datasets).

# Preparing the Dataset

### Downloading the Dataset

We used five open-source datasets in our paper:

| Data           | URL                                                                                                |
| -------------- | -------------------------------------------------------------------------------------------------- |
| alpaca-3.5-en  | https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json                            |
| alpaca-3.5-zh  | https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/dat                                        |
| alpaca-4-en    | https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data.json    |
| alpaca-4-zh    | https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json |
| sharegpt$^1$ | https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main                |

$^1$: Please note that the sharegpt dataset may continue to be updated, so it may differ slightly from what we used in the paper, but this does not significantly affect the experimental conclusions.

### Preprocessing

##### Cleaning the sharegpt dataset

We adopted the data-cleaning method from [Vicuna](https://github.com/lm-sys/FastChat/blob/main/docs/commands/data_cleaning.md):

1. Convert HTML to Markdown using `fastchat.data.clean_sharegpt`
2. Remove languages other than English and Chinese using `fastchat.data.optional_clean`
3. Split conversations longer than 2048 tokens using `fastchat.data.split_long_conversation`

#### Unifying the Data Format

We converted all datasets into the following format:

```python
{
    "id": "uniq_sample_id",
    "conversations": [
        {"from": "human", "value": "Hello"},
        {"from": "assistant", "value": "Hello, how may I help you?"},
        {"from": "human", "value": "How is the weather today?"},
        {"from": "assistant", "value": "Sorry, I cannot answer that because I don't know your location, and I currently cannot access real-time weather information."}
    ]
}