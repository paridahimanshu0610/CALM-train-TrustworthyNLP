Set the eos_token_id and bos_token_id of LLama model tokenizer
tokenizer.eos_token_id = 2
tokenizer.pad_token_id = 0
tokenizer.bos_token_id = 1
Loading lora config from /Users/himanshu/Documents/Projects/CALM-train-TrustworthyNLP/train/configs/lora_config_llama.json
Lora config: {'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'lora_target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'down_proj', 'gate_proj', 'up_proj']}
Eval tokenized example: {'id': 'CRA-422', 'conversations': [{'from': 'human', 'value': "Detect the credit card fraud with the following financial profile. Respond with only 'good' or 'bad', and do not provide any additional information. For instance, 'The client is a female, the state number is 25, the number of cards is 1, the credit balance is 7000, the number of transactions is 16, the number of international transactions is 0, the credit limit is 6.' should be classified as 'good'. \nText: 'The client is a female, the state number is 19, the number of cards is 1, the credit balance is 0, the number of transactions is 12, the number of international transactions is 0, the credit limit is 2.' \nAnswer:"}, {'from': 'assistant', 'value': 'good'}], 'input_ids': [12968, 29901, 29871, 13, 6362, 522, 278, 16200, 5881, 5227, 566, 411, 278, 1494, 18161, 8722, 29889, 2538, 2818, 411, 871, 525, 16773, 29915, 470, 525, 12313, 742, 322, 437, 451, 3867, 738, 5684, 2472, 29889, 1152, 2777, 29892, 525, 1576, 3132, 338, 263, 12944, 29892, 278, 2106, 1353, 338, 29871, 29906, 29945, 29892, 278, 1353, 310, 15889, 338, 29871, 29896, 29892, 278, 16200, 17346, 338, 29871, 29955, 29900, 29900, 29900, 29892, 278, 1353, 310, 22160, 338, 29871, 29896, 29953, 29892, 278, 1353, 310, 6121, 22160, 338, 29871, 29900, 29892, 278, 16200, 4046, 338, 29871, 29953, 6169, 881, 367, 770, 2164, 408, 525, 16773, 4286, 29871, 13, 1626, 29901, 525, 1576, 3132, 338, 263, 12944, 29892, 278, 2106, 1353, 338, 29871, 29896, 29929, 29892, 278, 1353, 310, 15889, 338, 29871, 29896, 29892, 278, 16200, 17346, 338, 29871, 29900, 29892, 278, 1353, 310, 22160, 338, 29871, 29896, 29906, 29892, 278, 1353, 310, 6121, 22160, 338, 29871, 29900, 29892, 278, 16200, 4046, 338, 29871, 29906, 6169, 29871, 13, 22550, 29901, 13, 13, 7900, 22137, 29901, 29871, 13, 1781, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1781, 2]}
Eval tokenized example: {'id': 'CRA-2635', 'conversations': [{'from': 'human', 'value': "Detect the credit card fraud using the following financial table attributes. Respond with only 'yes' or 'no', and do not provide any additional information. Therein, the data contains 28 numerical input variables V1, V2, ..., and V28 which are the result of a PCA transformation and 1 input variable Amount which has not been transformed with PCA. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. For instance, 'The client has attributes: V1: 0.144, V2: 0.358, V3: 1.220, V4: 0.331, V5: -0.273, V6: 0.429, V7: -0.307, V8: -0.577, V9: 0.116, V10: -0.337, V11: 1.016, V12: 1.043, V13: -0.527, V14: 0.160, V15: -0.951, V16: -0.452, V17: 0.166, V18: -0.446, V19: 0.036, V20: -0.275, V21: 0.768, V22: -0.051, V23: -0.180, V24: 0.067, V25: 0.741, V26: 0.477, V27: 0.152, V28: 0.201, Amount: 6.990.' should be classified as 'no'. \nText: 'The client has attributes: V1: -0.784, V2: 0.695, V3: 1.795, V4: 0.043, V5: -0.225, V6: -0.183, V7: 0.617, V8: 0.179, V9: 0.213, V10: -0.559, V11: -1.567, V12: -1.119, V13: -2.035, V14: 0.127, V15: -0.017, V16: -0.083, V17: -0.008, V18: -0.124, V19: 0.633, V20: 0.106, V21: -0.324, V22: -0.879, V23: -0.151, V24: -0.163, V25: 0.327, V26: 0.268, V27: 0.239, V28: 0.120, Amount: 47.150.' \nAnswer:"}, {'from': 'assistant', 'value': 'no'}], 'input_ids': [12968, 29901, 29871, 13, 6362, 522, 278, 16200, 5881, 5227, 566, 773, 278, 1494, 18161, 1591, 8393, 29889, 2538, 2818, 411, 871, 525, 3582, 29915, 470, 525, 1217, 742, 322, 437, 451, 3867, 738, 5684, 2472, 29889, 1670, 262, 29892, 278, 848, 3743, 29871, 29906, 29947, 16259, 1881, 3651, 478, 29896, 29892, 478, 29906, 29892, 2023, 29892, 322, 478, 29906, 29947, 607, 526, 278, 1121, 310, 263, 349, 5454, 13852, 322, 29871, 29896, 1881, 2286, 1913, 792, 607, 756, 451, 1063, 27615, 411, 349, 5454, 29889, 450, 4682, 525, 18087, 29915, 338, 278, 10804, 1913, 792, 29892, 445, 4682, 508, 367, 1304, 363, 1342, 29899, 2716, 5818, 3438, 29899, 23149, 3321, 6509, 29889, 1152, 2777, 29892, 525, 1576, 3132, 756, 8393, 29901, 478, 29896, 29901, 29871, 29900, 29889, 29896, 29946, 29946, 29892, 478, 29906, 29901, 29871, 29900, 29889, 29941, 29945, 29947, 29892, 478, 29941, 29901, 29871, 29896, 29889, 29906, 29906, 29900, 29892, 478, 29946, 29901, 29871, 29900, 29889, 29941, 29941, 29896, 29892, 478, 29945, 29901, 448, 29900, 29889, 29906, 29955, 29941, 29892, 478, 29953, 29901, 29871, 29900, 29889, 29946, 29906, 29929, 29892, 478, 29955, 29901, 448, 29900, 29889, 29941, 29900, 29955, 29892, 478, 29947, 29901, 448, 29900, 29889, 29945, 29955, 29955, 29892, 478, 29929, 29901, 29871, 29900, 29889, 29896, 29896, 29953, 29892, 478, 29896, 29900, 29901, 448, 29900, 29889, 29941, 29941, 29955, 29892, 478, 29896, 29896, 29901, 29871, 29896, 29889, 29900, 29896, 29953, 29892, 478, 29896, 29906, 29901, 29871, 29896, 29889, 29900, 29946, 29941, 29892, 478, 29896, 29941, 29901, 448, 29900, 29889, 29945, 29906, 29955, 29892, 478, 29896, 29946, 29901, 29871, 29900, 29889, 29896, 29953, 29900, 29892, 478, 29896, 29945, 29901, 448, 29900, 29889, 29929, 29945, 29896, 29892, 478, 29896, 29953, 29901, 448, 29900, 29889, 29946, 29945, 29906, 29892, 478, 29896, 29955, 29901, 29871, 29900, 29889, 29896, 29953, 29953, 29892, 478, 29896, 29947, 29901, 448, 29900, 29889, 29946, 29946, 29953, 29892, 478, 29896, 29929, 29901, 29871, 29900, 29889, 29900, 29941, 29953, 29892, 478, 29906, 29900, 29901, 448, 29900, 29889, 29906, 29955, 29945, 29892, 478, 29906, 29896, 29901, 29871, 29900, 29889, 29955, 29953, 29947, 29892, 478, 29906, 29906, 29901, 448, 29900, 29889, 29900, 29945, 29896, 29892, 478, 29906, 29941, 29901, 448, 29900, 29889, 29896, 29947, 29900, 29892, 478, 29906, 29946, 29901, 29871, 29900, 29889, 29900, 29953, 29955, 29892, 478, 29906, 29945, 29901, 29871, 29900, 29889, 29955, 29946, 29896, 29892, 478, 29906, 29953, 29901, 29871, 29900, 29889, 29946, 29955, 29955, 29892, 478, 29906, 29955, 29901, 29871, 29900, 29889, 29896, 29945, 29906, 29892, 478, 29906, 29947, 29901, 29871, 29900, 29889, 29906, 29900, 29896, 29892, 1913, 792, 29901, 29871, 29953, 29889, 29929, 29929, 29900, 6169, 881, 367, 770, 2164, 408, 525, 1217, 4286, 29871, 13, 1626, 29901, 525, 1576, 3132, 756, 8393, 29901, 478, 29896, 29901, 448, 29900, 29889, 29955, 29947, 29946, 29892, 478, 29906, 29901, 29871, 29900, 29889, 29953, 29929, 29945, 29892, 478, 29941, 29901, 29871, 29896, 29889, 29955, 29929, 29945, 29892, 478, 29946, 29901, 29871, 29900, 29889, 29900, 29946, 29941, 29892, 478, 29945, 29901, 448, 29900, 29889, 29906, 29906, 29945, 29892, 478, 29953, 29901, 448, 29900, 29889, 29896, 29947, 29941, 29892, 478, 29955, 29901, 29871, 29900, 29889, 29953, 29896, 29955, 29892, 478, 29947, 29901, 29871, 29900, 29889, 29896, 29955, 29929, 29892, 478, 29929, 29901, 29871, 29900, 29889, 29906, 29896, 29941, 29892, 478, 29896, 29900, 29901, 448, 29900, 29889, 29945, 29945, 29929, 29892, 478, 29896, 29896, 29901, 448, 29896, 29889, 29945, 29953, 29955, 29892, 478, 29896, 29906, 29901, 448, 29896, 29889, 29896, 29896, 29929, 29892, 478, 29896, 29941, 29901, 448, 29906, 29889, 29900, 29941, 29945, 29892, 478, 29896, 29946, 29901, 29871, 29900, 29889, 29896, 29906, 29955, 29892, 478, 29896, 29945, 29901, 448, 29900, 29889, 29900, 29896, 29955, 29892, 478, 29896, 29953, 29901, 448, 29900, 29889, 29900, 29947, 29941, 29892, 478, 29896, 29955, 29901, 448, 29900, 29889, 29900, 29900, 29947, 29892, 478, 29896, 29947, 29901, 448, 29900, 29889, 29896, 29906, 29946, 29892, 478, 29896, 29929, 29901, 29871, 29900, 29889, 29953, 29941, 29941, 29892, 478, 29906, 29900, 29901, 29871, 29900, 29889, 29896, 29900, 29953, 29892, 478, 29906, 29896, 29901, 448, 29900, 29889, 29941, 29906, 29946, 29892, 478, 29906, 29906, 29901, 448, 29900, 29889, 29947, 29955, 29929, 29892, 478, 29906, 29941, 29901, 448, 29900, 29889, 29896, 29945, 29896, 29892, 478, 29906, 29946, 29901, 448, 29900, 29889, 29896, 29953, 29941, 29892, 478, 29906, 29945, 29901, 29871, 29900, 29889, 29941, 29906, 29955, 29892, 478, 29906, 29953, 29901, 29871, 29900, 29889, 29906, 29953, 29947, 29892, 478, 29906, 29955, 29901, 29871, 29900, 29889, 29906, 29941, 29929, 29892, 478, 29906, 29947, 29901, 29871, 29900, 29889, 29896, 29906, 29900, 29892, 1913, 792, 29901, 29871, 29946, 29955, 29889, 29896, 29945, 29900, 6169, 29871, 13, 22550, 29901, 13, 13, 7900, 22137, 29901, 29871, 13, 694, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 2]}
Train tokenized example: {'id': 'CRA-37069', 'conversations': [{'from': 'human', 'value': "Identify the claim status of insurance companies using the following table attributes for travel insurance status. Respond with only 'yes' or 'no', and do not provide any additional information. And the table attributes including 5 categorical attributes and 4 numerical attributes are as follows: \nAgency: Name of agency (categorical). \nAgency Type: Type of travel insurance agencies (categorical). \nDistribution Channel: Distribution channel of travel insurance agencies (categorical). \nProduct Name: Name of the travel insurance products (categorical). \nDuration: Duration of travel (categorical). \nDestination: Destination of travel (numerical). \nNet Sales: Amount of sales of travel insurance policies (categorical). \nCommission: Commission received for travel insurance agency (numerical). \nAge: Age of insured (numerical). \nFor instance: 'The insurance company has attributes: Agency: CBH, Agency Type: Travel Agency, Distribution Chanel: Offline, Product Name: Comprehensive Plan, Duration: 186, Destination: MALAYSIA, Net Sales: -29, Commision: 9.57, Age: 81.', should be classified as 'no'. \nText: 'The insurance company has attributes: Agency: EPX, Agency Type: Travel Agency, Distribution Channel: Online, Product Name: Cancellation Plan, Duration: 7, Destination: CAMBODIA, Net Sales: 17.0, Commission: 0.0, Age: 36.' \nAnswer:"}, {'from': 'assistant', 'value': 'no'}], 'input_ids': [12968, 29901, 29871, 13, 7648, 1598, 278, 5995, 4660, 310, 1663, 18541, 14582, 773, 278, 1494, 1591, 8393, 363, 9850, 1663, 18541, 4660, 29889, 2538, 2818, 411, 871, 525, 3582, 29915, 470, 525, 1217, 742, 322, 437, 451, 3867, 738, 5684, 2472, 29889, 1126, 278, 1591, 8393, 3704, 29871, 29945, 11608, 936, 8393, 322, 29871, 29946, 16259, 8393, 526, 408, 4477, 29901, 29871, 13, 29909, 14703, 29901, 4408, 310, 946, 3819, 313, 29883, 20440, 936, 467, 29871, 13, 29909, 14703, 5167, 29901, 5167, 310, 9850, 1663, 18541, 946, 15942, 313, 29883, 20440, 936, 467, 29871, 13, 13398, 3224, 17368, 29901, 17740, 8242, 310, 9850, 1663, 18541, 946, 15942, 313, 29883, 20440, 936, 467, 29871, 13, 7566, 4408, 29901, 4408, 310, 278, 9850, 1663, 18541, 9316, 313, 29883, 20440, 936, 467, 29871, 13, 18984, 29901, 360, 2633, 310, 9850, 313, 29883, 20440, 936, 467, 29871, 13, 14994, 3381, 29901, 15435, 3381, 310, 9850, 313, 8058, 936, 467, 29871, 13, 6779, 28389, 29901, 1913, 792, 310, 16538, 310, 9850, 1663, 18541, 24833, 313, 29883, 20440, 936, 467, 29871, 13, 5261, 2333, 29901, 11444, 4520, 363, 9850, 1663, 18541, 946, 3819, 313, 8058, 936, 467, 29871, 13, 22406, 29901, 16767, 310, 1663, 2955, 313, 8058, 936, 467, 29871, 13, 2831, 2777, 29901, 525, 1576, 1663, 18541, 5001, 756, 8393, 29901, 29353, 29901, 315, 29933, 29950, 29892, 29353, 5167, 29901, 3201, 955, 29353, 29892, 17740, 678, 3870, 29901, 5947, 1220, 29892, 10969, 4408, 29901, 422, 1457, 29882, 6270, 8402, 29892, 360, 2633, 29901, 29871, 29896, 29947, 29953, 29892, 15435, 3381, 29901, 341, 1964, 29909, 29979, 5425, 29909, 29892, 12670, 28389, 29901, 448, 29906, 29929, 29892, 1876, 2459, 29901, 29871, 29929, 29889, 29945, 29955, 29892, 16767, 29901, 29871, 29947, 29896, 29889, 742, 881, 367, 770, 2164, 408, 525, 1217, 4286, 29871, 13, 1626, 29901, 525, 1576, 1663, 18541, 5001, 756, 8393, 29901, 29353, 29901, 16502, 29990, 29892, 29353, 5167, 29901, 3201, 955, 29353, 29892, 17740, 17368, 29901, 13542, 29892, 10969, 4408, 29901, 1815, 22603, 8402, 29892, 360, 2633, 29901, 29871, 29955, 29892, 15435, 3381, 29901, 315, 5194, 8456, 4571, 29909, 29892, 12670, 28389, 29901, 29871, 29896, 29955, 29889, 29900, 29892, 11444, 29901, 29871, 29900, 29889, 29900, 29892, 16767, 29901, 29871, 29941, 29953, 6169, 29871, 13, 22550, 29901, 13, 13, 7900, 22137, 29901, 29871, 13, 694, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 2]}
Train tokenized example: {'id': 'CRA-8598', 'conversations': [{'from': 'human', 'value': "Detect the credit card fraud with the following financial profile. Respond with only 'good' or 'bad', and do not provide any additional information. For instance, 'The client is a female, the state number is 25, the number of cards is 1, the credit balance is 7000, the number of transactions is 16, the number of international transactions is 0, the credit limit is 6.' should be classified as 'good'. \nText: 'The client is a female, the state number is 42, the number of cards is 1, the credit balance is 3912, the number of transactions is 60, the number of international transactions is 0, the credit limit is 12.' \nAnswer:"}, {'from': 'assistant', 'value': 'bad'}], 'input_ids': [12968, 29901, 29871, 13, 6362, 522, 278, 16200, 5881, 5227, 566, 411, 278, 1494, 18161, 8722, 29889, 2538, 2818, 411, 871, 525, 16773, 29915, 470, 525, 12313, 742, 322, 437, 451, 3867, 738, 5684, 2472, 29889, 1152, 2777, 29892, 525, 1576, 3132, 338, 263, 12944, 29892, 278, 2106, 1353, 338, 29871, 29906, 29945, 29892, 278, 1353, 310, 15889, 338, 29871, 29896, 29892, 278, 16200, 17346, 338, 29871, 29955, 29900, 29900, 29900, 29892, 278, 1353, 310, 22160, 338, 29871, 29896, 29953, 29892, 278, 1353, 310, 6121, 22160, 338, 29871, 29900, 29892, 278, 16200, 4046, 338, 29871, 29953, 6169, 881, 367, 770, 2164, 408, 525, 16773, 4286, 29871, 13, 1626, 29901, 525, 1576, 3132, 338, 263, 12944, 29892, 278, 2106, 1353, 338, 29871, 29946, 29906, 29892, 278, 1353, 310, 15889, 338, 29871, 29896, 29892, 278, 16200, 17346, 338, 29871, 29941, 29929, 29896, 29906, 29892, 278, 1353, 310, 22160, 338, 29871, 29953, 29900, 29892, 278, 1353, 310, 6121, 22160, 338, 29871, 29900, 29892, 278, 16200, 4046, 338, 29871, 29896, 29906, 6169, 29871, 13, 22550, 29901, 13, 13, 7900, 22137, 29901, 29871, 13, 4319, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4319, 2]}
num_gpus = 0, training_nums = 44863, t_total = 112160.0, warmup_steps = 1121, eval_steps = 22432.0, save_steps = 22432.0
val data nums = 3000, training_nums = 44863, batch_size = 2
Using auto half precision backend
***** Running training *****
  Num examples = 44863
  Num train samples = 224315.0
  world_size = 1
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 2
  Total optimization steps = 112155
  Number of trainable parameters = 39976960
